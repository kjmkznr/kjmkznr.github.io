<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ただのメモ</title>
    <link>https://kjmkznr.github.io/post/index.xml</link>
    <description>Recent content in Posts on ただのメモ</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Oct 2017 16:00:00 +0000</lastBuildDate>
    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ISUCON7 予選参戦メモ</title>
      <link>https://kjmkznr.github.io/post/20171026/isucon7-qual/</link>
      <pubDate>Thu, 26 Oct 2017 16:00:00 +0000</pubDate>
      
      <guid>https://kjmkznr.github.io/post/20171026/isucon7-qual/</guid>
      <description>&lt;p&gt;2017/10/21〜22 に開催された ISUCON7 の予選に参加して、予選突破できたので、そのときのメモです。
22日（日曜）の方に参加しました。&lt;/p&gt;

&lt;h2 id=&#34;チーム構成&#34;&gt;チーム構成&lt;/h2&gt;

&lt;h3 id=&#34;チーム名&#34;&gt;チーム名&lt;/h3&gt;

&lt;p&gt;oops&lt;/p&gt;

&lt;h3 id=&#34;メンバー&#34;&gt;メンバー&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;2matz(インフラ、アプリ）&lt;/li&gt;
&lt;li&gt;nyaa（アプリ、インフラ）&lt;/li&gt;
&lt;li&gt;kjm（インフラ、アプリ）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;今は全員&lt;a href=&#34;http://www.4cast.co.jp/recruit/recruit/carrier.html&#34;&gt;同僚&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;選択言語&#34;&gt;選択言語&lt;/h3&gt;

&lt;p&gt;Go言語&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;点数の推移&#34;&gt;点数の推移&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;https://kjmkznr.github.io/images/20171026/isucon7-qual-1.png&#34; alt=&#34;oops のみのグラフ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;16時くらいまでは暫定一位だった。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://kjmkznr.github.io/images/20171026/isucon7-qual-2.png&#34; alt=&#34;予選通過チームを含むグラフ&#34; /&gt;&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;時間&lt;/th&gt;
&lt;th&gt;Score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;13:05:29&lt;/td&gt;
&lt;td&gt;3,468&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13:15:12&lt;/td&gt;
&lt;td&gt;6,036&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13:47:44&lt;/td&gt;
&lt;td&gt;17,721&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;13:53:41&lt;/td&gt;
&lt;td&gt;19,535&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:00:59&lt;/td&gt;
&lt;td&gt;1,733&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:02:11&lt;/td&gt;
&lt;td&gt;27,345&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:18:39&lt;/td&gt;
&lt;td&gt;19,320&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:29:11&lt;/td&gt;
&lt;td&gt;26,212&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:30:58&lt;/td&gt;
&lt;td&gt;82,260&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:32:33&lt;/td&gt;
&lt;td&gt;85,310&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:49:54&lt;/td&gt;
&lt;td&gt;36,704&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;14:52:45&lt;/td&gt;
&lt;td&gt;27,995&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:18:13&lt;/td&gt;
&lt;td&gt;100,117&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:22:43&lt;/td&gt;
&lt;td&gt;69,093&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:29:11&lt;/td&gt;
&lt;td&gt;31,157&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:38:11&lt;/td&gt;
&lt;td&gt;65,097&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:39:40&lt;/td&gt;
&lt;td&gt;68,131&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:42:03&lt;/td&gt;
&lt;td&gt;106,396&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;15:48:01&lt;/td&gt;
&lt;td&gt;98,140&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:03:03&lt;/td&gt;
&lt;td&gt;85,097&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:08:42&lt;/td&gt;
&lt;td&gt;142,119&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:18:00&lt;/td&gt;
&lt;td&gt;30,340&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:22:39&lt;/td&gt;
&lt;td&gt;57,593&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:26:21&lt;/td&gt;
&lt;td&gt;44,667&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:30:43&lt;/td&gt;
&lt;td&gt;38,876&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:32:31&lt;/td&gt;
&lt;td&gt;67,046&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:37:13&lt;/td&gt;
&lt;td&gt;37,649&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:42:17&lt;/td&gt;
&lt;td&gt;46,491&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;16:45:52&lt;/td&gt;
&lt;td&gt;32,706&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:01:03&lt;/td&gt;
&lt;td&gt;91,630&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:04:12&lt;/td&gt;
&lt;td&gt;58,803&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:17:53&lt;/td&gt;
&lt;td&gt;70,538&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:22:52&lt;/td&gt;
&lt;td&gt;30,633&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:25:25&lt;/td&gt;
&lt;td&gt;43,560&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:31:02&lt;/td&gt;
&lt;td&gt;69,867&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;17:50:31&lt;/td&gt;
&lt;td&gt;162,094&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:00:43&lt;/td&gt;
&lt;td&gt;127,168&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:02:54&lt;/td&gt;
&lt;td&gt;127,999&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:04:09&lt;/td&gt;
&lt;td&gt;109,519&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:12:04&lt;/td&gt;
&lt;td&gt;123,991&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:17:29&lt;/td&gt;
&lt;td&gt;63,554&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:20:45&lt;/td&gt;
&lt;td&gt;48,520&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:23:24&lt;/td&gt;
&lt;td&gt;181,188&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:27:27&lt;/td&gt;
&lt;td&gt;158,768&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:39:22&lt;/td&gt;
&lt;td&gt;163,762&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;18:45:01&lt;/td&gt;
&lt;td&gt;189,698&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:08:04&lt;/td&gt;
&lt;td&gt;98,758&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:09:15&lt;/td&gt;
&lt;td&gt;70,864&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:12:53&lt;/td&gt;
&lt;td&gt;69,786&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:15:39&lt;/td&gt;
&lt;td&gt;69,863&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:26:00&lt;/td&gt;
&lt;td&gt;208746&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:38:05&lt;/td&gt;
&lt;td&gt;215,780&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;19:56:24&lt;/td&gt;
&lt;td&gt;184,449&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:03:47&lt;/td&gt;
&lt;td&gt;202,946&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:08:39&lt;/td&gt;
&lt;td&gt;175,430&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:13:03&lt;/td&gt;
&lt;td&gt;161,017&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:15:08&lt;/td&gt;
&lt;td&gt;174,941&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:29:59&lt;/td&gt;
&lt;td&gt;146,447&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:31:13&lt;/td&gt;
&lt;td&gt;217,747&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:35:43&lt;/td&gt;
&lt;td&gt;199,209&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:38:50&lt;/td&gt;
&lt;td&gt;183,515&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:40:36&lt;/td&gt;
&lt;td&gt;146,219&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:43:27&lt;/td&gt;
&lt;td&gt;179,772&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:46:04&lt;/td&gt;
&lt;td&gt;185,481&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:49:13&lt;/td&gt;
&lt;td&gt;215,451&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:50:39&lt;/td&gt;
&lt;td&gt;195,725&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:51:47&lt;/td&gt;
&lt;td&gt;167,884&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;20:53:04&lt;/td&gt;
&lt;td&gt;216,923&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Best 217,747&lt;/li&gt;
&lt;li&gt;Last 216,923&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;結果&#34;&gt;結果&lt;/h2&gt;

&lt;p&gt;両日3チームを除いた上位12チームにおいて、8番目(全体で14位)でなんとかギリギリ本戦出場権を得られました。
ISUCON4 以来の2回目です。
ちなみに ISUCON4 のときも総合トップ13枠で8位だった。&lt;/p&gt;

&lt;h2 id=&#34;使用したツール&#34;&gt;使用したツール&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;kataribe

&lt;ul&gt;
&lt;li&gt;matsuu++&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;tcpdump&lt;/li&gt;
&lt;li&gt;Wireshark&lt;/li&gt;
&lt;li&gt;goreplay

&lt;ul&gt;
&lt;li&gt;今回は複数台サーバがあったのであまり使わなかったが、ローカルでリクエストをリプレイしてアプリの挙動に問題ないか確認に使おうと思っていた。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;netdata&lt;/li&gt;
&lt;li&gt;NewRelic

&lt;ul&gt;
&lt;li&gt;Go のライブラリだとあまり意味がなかった&lt;/li&gt;
&lt;li&gt;Runtime GC のところだけチラっと見たくらい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;BitBucket&lt;/li&gt;
&lt;li&gt;ansible&lt;/li&gt;
&lt;li&gt;mosh&lt;/li&gt;
&lt;li&gt;Dropbox Paper&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;やったこと&#34;&gt;やったこと&lt;/h2&gt;

&lt;h3 id=&#34;前日まで&#34;&gt;前日まで&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Dropbox Paper で当日のスケジュールをおおまかに決め共有&lt;/li&gt;
&lt;li&gt;Pixiv ISUCON で勉強&lt;/li&gt;
&lt;li&gt;当日のお昼ご飯

&lt;ul&gt;
&lt;li&gt;事前に出前を予約。 &lt;img src=&#34;https://kjmkznr.github.io/images/20171026/launch.jpg&#34; alt=&#34;Launch&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;当日&#34;&gt;当日&lt;/h3&gt;

&lt;h4 id=&#34;序盤&#34;&gt;序盤&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;初期セットアップ

&lt;ul&gt;
&lt;li&gt;ソースコードを git にぶっこんでサーバで git pull が行えるようにする&lt;/li&gt;
&lt;li&gt;ツールのインストールやSSH鍵を登録する Ansible Playbook をサーバに展開、適用&lt;/li&gt;
&lt;li&gt;fail2ban 無効化

&lt;ul&gt;
&lt;li&gt;事前に Ubuntu を試したときにハマったメンバーがいたので。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;画像のスタティックファイル化

&lt;ul&gt;
&lt;li&gt;nyaa がさくっと実装&lt;/li&gt;
&lt;li&gt;pixxiv-isucon 予習の成果&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;nginx の設定をチューニング1

&lt;ul&gt;
&lt;li&gt;Expires, Cache-Control ヘッダまわりを追加&lt;/li&gt;
&lt;li&gt;ETag 無効化&lt;/li&gt;
&lt;li&gt;これも nyaa がさくっと設定&lt;/li&gt;
&lt;li&gt;pixxiv-isucon 予習の成果&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;message テーブルにインデックスを張る

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ALTER TABLE message ADD INDEX(channel_id);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;これも nyaa&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;/message のチューニング

&lt;ul&gt;
&lt;li&gt;N+1 を改善&lt;/li&gt;
&lt;li&gt;これも nyaa&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;/fetch のチューニング

&lt;ul&gt;
&lt;li&gt;私が担当したが N+1 のクエリの排除までいけず。&lt;/li&gt;
&lt;li&gt;haveread を Redis 化した方が良いよなーとつぶやいてたけど、結局実行せず。&lt;/li&gt;
&lt;li&gt;nyaa にバトンタッチして改善&lt;/li&gt;
&lt;li&gt;time.Sleep を消したりもしてみたが、リクエスト数が増えるだけで効果がないので戻した&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;このあたりのベストスコアが142,119点だったけど、なかなか安定せず&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;中盤&#34;&gt;中盤&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;message テーブルにさらにインデックスを追加

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ALTER TABLE message ADD INDEX(user_id);&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;最終的に &lt;code&gt;ALTER TABLE message ADD INDEX(channel_id, user_id);&lt;/code&gt; というインデックスに集約。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;複数台構成への変更

&lt;ul&gt;
&lt;li&gt;いくつか案が出た

&lt;ul&gt;
&lt;li&gt;没案としては NFS や WebDav など。&lt;/li&gt;
&lt;li&gt;NFS は再起動時に刺さりそうだったのでやめた&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;/icons 以下を3台のサーバに分散配置し、ローカルにない場合は存在するノードにリダイレクトするような処理を2matzが実装

&lt;ul&gt;
&lt;li&gt;残念ながらベンチマーカーがリダイレクトしてくれなかったのでロールバック&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;nginx のみで複数台構成への切り替え

&lt;ul&gt;
&lt;li&gt;私が担当した。&lt;/li&gt;
&lt;li&gt;上で書いた分散配置の施策は変更が大きくなりそうだったため、予備の案を考えておいた&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;app1, app2&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;server {
        location /icons/ {
                proxy_pass http://app3;
        }
        location / {
                proxy_pass http://127.0.0.1:5000;
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;app3&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;upstream backend {                                                              
        server app1:5000;
        server app2:5000;
}
server {
        location /icons/ {
                add_header Pragma public;
                add_header Cache-Control &amp;quot;public, must-revalidate, proxy-revalidate&amp;quot;;
                expires max;
                etag off;
        }
        location /profile {
                proxy_pass http://127.0.0.1:5000;
        }
        location / {
                proxy_pass http://backend;
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;終盤&#34;&gt;終盤&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;app1, app2 に proxy_cache の設定を追加&lt;/li&gt;
&lt;li&gt;/history を改善&lt;/li&gt;
&lt;li&gt;サーバの再起動テスト

&lt;ul&gt;
&lt;li&gt;1台ずつ再起動し無事起動してくることを祈る&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;各種ログの停止&lt;/li&gt;
&lt;li&gt;MySQL のチューニング

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;innodb_doublewrite = 0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;innodb_flush_log_at_trx_commit = 2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;ベンチガチャ

&lt;ul&gt;
&lt;li&gt;Goのアプリのメモリが肥大化するので、ベンチのたびに再起動させたり。&lt;/li&gt;
&lt;li&gt;ベストスコアに近い値を出すまで繰替えしベンチ&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;祈り&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;まとめと反省&#34;&gt;まとめと反省&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;だいたい nyaa の成果。

&lt;ul&gt;
&lt;li&gt;nyaa に感謝&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;「推測するな計測しろ」の原則をあまり守れなかった

&lt;ul&gt;
&lt;li&gt;netdata を使ってウォッチしていたが、大まかにしか見れていなかった&lt;/li&gt;
&lt;li&gt;ISUCON の場合 Mackerel などで見れる 1分単位のメトリクスだと粒度が粗いので netdata を入れて見るのが楽。

&lt;ul&gt;
&lt;li&gt;ただ複数台をウォッチしにくい&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://kjmkznr.github.io/post/consul-prometheus/&#34;&gt;2015年の記事で使用している Prometheus&lt;/a&gt;とGrafanaでダッシュボードを用意したかった&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;毎度のことながら SQL 力の不足&lt;/li&gt;
&lt;li&gt;毎度のことながら コミュニケーション不足

&lt;ul&gt;
&lt;li&gt;前回より大分マシだった&lt;/li&gt;
&lt;li&gt;座席の位置とかも重要かもしれない&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;app3 に負荷が集中する構成だったのは良くなかった

&lt;ul&gt;
&lt;li&gt;app1, app2 だけでベンチを走らせてみればよかった&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ベンチマーカーのエラー「too many connections」の発生条件がよくわからなかった&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;同時接続数の問題なんだろうけれど、ベンチマーカのバグなのか、仕様なのか判断がつかなかった。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;10/22 18:27:20 エラーが発生したため負荷レベルを上げられませんでした。2017-10-22 18:27:19.812958767 +0900 JST m=+56.143739094 リクエストに失敗しました Post http://isubata.example.com/message: dial tcp 59.106.218.178:80: socket: too many open files (POST /message )&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ルールを熟読しよう&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;どのようなリクエストのポイントが高いのか、ルールに書かれているのならば把握して意識すべき&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;最後に&#34;&gt;最後に&lt;/h2&gt;

&lt;p&gt;pixiv-isucon を作成、公開してくださった catatsuyさん、毎回大変な準備をされている運営の皆様には深く御礼申し上げます。
本戦がんばろう。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Terraform: AWSマルチリージョンな環境を構築してみる</title>
      <link>https://kjmkznr.github.io/post/20151205/terraform-multi-region/</link>
      <pubDate>Sat, 05 Dec 2015 13:00:00 +0900</pubDate>
      
      <guid>https://kjmkznr.github.io/post/20151205/terraform-multi-region/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://qiita.com/advent-calendar/2015/hashicorp&#34;&gt;HashiCorp Advent Calendar 2015&lt;/a&gt;の5日目の担当が空いていたので書いてみました。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;クラウドを利用されているみなさんは、リージョン障害などに備えるために複数のリージョンを利用されているかと思います。
複数のリージョンに展開する際、どのような方法で行ってますでしょうか？&lt;/p&gt;

&lt;p&gt;CloudFormation Stackを複数のリージョンで作ったり、手動でポチポチやったり、SDKやAWSCLIを使ったスクリプトを使ったり&amp;hellip;&lt;/p&gt;

&lt;p&gt;Terraform を使えば簡単に複数のリージョンを統一的に扱うことが出来ます。&lt;/p&gt;

&lt;p&gt;Terraform は複数のプロバイダ(AWSやGCPなど)を一つのテンプレートの中に混在することが出来ます。
また、1種類のプロバイダを複数使用することも出来るため、AWSにマルチリージョンな環境を構築できます。&lt;/p&gt;

&lt;p&gt;AWS謹製のCloudFormationは、S3やRoute53などのグローバルリソースを除き、そのCloudFormation Stackを作成しているリージョンのリソースしか作成することが出来ません。
もちろん、GCPやDigitalOceanなど外部のサービスを利用することも出来ません。&lt;/p&gt;

&lt;p&gt;このあたりは Terraform の強みですね。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;マルチリージョン基本編&#34;&gt;マルチリージョン基本編&lt;/h2&gt;

&lt;p&gt;まずは基本的な複数のプロバイダの使い方についてです。&lt;/p&gt;

&lt;p&gt;シングルリージョンでAWSを利用する場合は以下のように記述します。
&lt;code&gt;aws_&lt;/code&gt; で始まるリソースは &lt;code&gt;aws&lt;/code&gt; プロバイダが利用されます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;provider &amp;quot;aws&amp;quot; {
    region = &amp;quot;ap-northeast-1&amp;quot;
}

resource &amp;quot;aws_instance&amp;quot; &amp;quot;instance-tokyo&amp;quot; {
    instance_type = &amp;quot;t2.micro&amp;quot;
    ami           = &amp;quot;ami-383c1956&amp;quot;
    tag {
        Name = &amp;quot;Tokyo Region&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;マルチリージョンで利用したい場合、&lt;code&gt;provider &amp;quot;aws&amp;quot;&lt;/code&gt; を複数書けば良いというわけではありません。
と言ってもそんなに難しいことをやるわけではなく、どちらのプロバイダを利用するのかを識別するために&lt;code&gt;alias&lt;/code&gt;を追加するだけです。&lt;/p&gt;

&lt;p&gt;あとは Resource の定義でどのプロバイダを使用するかを&lt;code&gt;provider&lt;/code&gt;で指定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;provider &amp;quot;aws&amp;quot; {
    region = &amp;quot;ap-northeast-1&amp;quot;
    alias = &amp;quot;tokyo&amp;quot;
}
provider &amp;quot;aws&amp;quot; {
    region = &amp;quot;ap-southeast-1&amp;quot;
    alias = &amp;quot;singapore&amp;quot;
}

resource &amp;quot;aws_instance&amp;quot; &amp;quot;instance-tokyo&amp;quot; {
    provider      = &amp;quot;aws.tokyo&amp;quot;
    instance_type = &amp;quot;t2.micro&amp;quot;
    ami           = &amp;quot;ami-383c1956&amp;quot;
    tag {
        Name = &amp;quot;Tokyo Region&amp;quot;
    }
}
resource &amp;quot;aws_instance&amp;quot; &amp;quot;instance-singapore&amp;quot; {
    provider      = &amp;quot;aws.singapore&amp;quot;
    instance_type = &amp;quot;t2.micro&amp;quot;
    ami           = &amp;quot;ami-c9b572aa&amp;quot;
    tag {
        Name = &amp;quot;Singapore Region&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;マルチリージョン応用編&#34;&gt;マルチリージョン応用編&lt;/h2&gt;

&lt;p&gt;マルチリージョンにする大きな理由として、冗長性の確保が挙げられると思います。
環境を冗長構成する場合、同じ物をスタンプのように複数のリージョンに展開していくかと思いますが、前述の方法では&lt;code&gt;provider&lt;/code&gt;と&lt;code&gt;ami&lt;/code&gt;だけが違うリソースを大量に定義していかなければならず、非常にめんどくさいですし、修正時に一部のリージョンを変更し忘れるといったオペミスも発生しやすくなります。&lt;/p&gt;

&lt;p&gt;リージョン毎に同じリソースを展開する場合、共通する部分をモジュールとしてまとめてしまうのが簡単です。&lt;/p&gt;

&lt;p&gt;今回サンプルとして作成するテンプレートのディレクトリ構成は以下のようにします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;./global.tf
./main.tf
./regional-resource/main.tf
./regional-resource/variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;モジュール側のリソース定義&#34;&gt;モジュール側のリソース定義&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;./regional-resource/main.tf
```
provider &amp;ldquo;aws&amp;rdquo; {
region = &amp;ldquo;${var.region}&amp;rdquo;
}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;resource &amp;ldquo;aws_instance&amp;rdquo; &amp;ldquo;instance&amp;rdquo; {
    instance_type               = &amp;ldquo;t2.micro&amp;rdquo;
    ami                         = &amp;ldquo;${lookup(var.ami_id, var.region)}&amp;rdquo;
    associate_public_ip_address = true
    tag {
        Name = &amp;ldquo;${var.name}&amp;rdquo;
    }
}&lt;/p&gt;

&lt;p&gt;output &amp;ldquo;public_ip&amp;rdquo; {
    value = &amp;ldquo;${aws_instance.instance.public_ip}&amp;rdquo;
}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
* ./regional-resource/variables.tf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;variable &amp;ldquo;region&amp;rdquo; {}
variable &amp;ldquo;name&amp;rdquo; {}
variable &amp;ldquo;ami_id&amp;rdquo; {
    default {
        ap-northeast-1  = &amp;ldquo;ami-383c1956&amp;rdquo;
        ap-southeast-1  = &amp;ldquo;ami-c9b572aa&amp;rdquo;
        ap-southeast-2  = &amp;ldquo;ami-48d38c2b&amp;rdquo;
        us-east-1       = &amp;ldquo;ami-60b6c60a&amp;rdquo;
        us-west-1       = &amp;ldquo;ami-d5ea86b5&amp;rdquo;
        us-west-2       = &amp;ldquo;ami-f0091d91&amp;rdquo;
        eu-west-1       = &amp;ldquo;ami-bff32ccc&amp;rdquo;
        eu-central-1    = &amp;ldquo;ami-bc5b48d0&amp;rdquo;
        sa-east-1       = &amp;ldquo;ami-6817af04&amp;rdquo;
    }
}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
### モジュールを使う側のリソース定義

* ./main.tf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;module &amp;ldquo;ap-northeast-1&amp;rdquo; {
    source = &amp;ldquo;./regional-resource&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;region   = &amp;quot;ap-northeast-1&amp;quot;
name     = &amp;quot;Tokyo&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}
module &amp;ldquo;ap-southeast-1&amp;rdquo; {
    source = &amp;ldquo;./regional-resource&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;region   = &amp;quot;ap-southeast-1&amp;quot;
name     = &amp;quot;Singapore&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
* ./global.tf

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;resource &amp;ldquo;aws_route53_zone&amp;rdquo; &amp;ldquo;primary&amp;rdquo; {
   name = &amp;ldquo;example.com&amp;rdquo;
}&lt;/p&gt;

&lt;p&gt;resource &amp;ldquo;aws_route53_record&amp;rdquo; &amp;ldquo;www&amp;rdquo; {
   zone_id = &amp;ldquo;${aws_route53_zone.primary.zone_id}&amp;rdquo;
   name = &amp;ldquo;www.example.com&amp;rdquo;
   type = &amp;ldquo;A&amp;rdquo;
   ttl = &amp;ldquo;300&amp;rdquo;
   records = [
       &amp;ldquo;${module.ap-northeast-1.public_ip}&amp;ldquo;,
       &amp;ldquo;${module.ap-southeast-1.public_ip}&amp;ldquo;,
   ]
}&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
`global.tf` ではRoute53にゾーンを作成し、東京都シンガポールのインスタンスのIPアドレスを登録しています。

以上のテンプレートで出来上がる構成がこんな感じです。

![multi region diagram](/images/20151205/terraform_multi-region.png)

### Provision
モジュールを使うと、単に `terraform apply` とするだけではダメで先に `terraform get` を行う必要があります。

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$ terraform get -update=true
$ terraform plan -module-depth=-1
$ terraform apply
```&lt;/p&gt;

&lt;p&gt;今回はローカルファイルシステムにあるモジュールを使いましたが、Gitなどからも取ってくることが出来ます。(&lt;a href=&#34;https://terraform.io/docs/modules/usage.html&#34;&gt;Using Modules&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;このように、Terraformを使えば簡単にマルチリージョンな環境を手に入れることができます。
マルチリージョンどころかマルチクラウドな環境も、今回よりちょっと大変ではありますが実現できます。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>VaultのSSH Secret Backendを使ってみる</title>
      <link>https://kjmkznr.github.io/post/vault-ssh-secret-backend/</link>
      <pubDate>Fri, 23 Oct 2015 01:18:00 +0900</pubDate>
      
      <guid>https://kjmkznr.github.io/post/vault-ssh-secret-backend/</guid>
      <description>&lt;p&gt;Vault 0.3がリリースされたときにSSHに関する機能が実装されたと言うのを見て気になっていたのですが、なかなか試せずにいたのですが、先日&lt;a href=&#34;http://connpass.com/event/19980/&#34;&gt;HashiCode#2&lt;/a&gt;に参加したので、その勢いで試してみました。&lt;/p&gt;

&lt;p&gt;Vault の SSH Secret Backend はもの凄く大ざっぱに言うとVaultがSSHのパスワードや公開鍵の管理を行ってくれる機能です。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;使うもの&#34;&gt;使うもの&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Vault 0.3以上&lt;/li&gt;
&lt;li&gt;OpenSSH&lt;/li&gt;
&lt;li&gt;vault-ssh-helper&lt;/li&gt;
&lt;li&gt;CentOS 7.0

&lt;ul&gt;
&lt;li&gt;ディストリはGentooでも何でも&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;ssh-secret-backend&#34;&gt;SSH Secret Backend&lt;/h2&gt;

&lt;p&gt;まずは Vault について理解を深めるために&lt;a href=&#34;https://vaultproject.io/intro/getting-started/install.html&#34;&gt;Getting Started&lt;/a&gt;を行ってみるとよいと思います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vault server -dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このように&lt;code&gt;-dev&lt;/code&gt;を付けて起動すると何も考えずに各種機能を試せます。
この dev モードはメモリだけで動作するので、終了すると設定した内容はすべて消えてしまいます。
また、データの seal もされてないので、プロダクション環境では使わないようにしましょう。&lt;/p&gt;

&lt;h3 id=&#34;vault-ssh-helper-のインストール&#34;&gt;vault-ssh-helper のインストール&lt;/h3&gt;

&lt;p&gt;Vault の操作の前に vault-ssh-helper をインストールする必要があります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/hashicorp/vault-ssh-helper
$ cd vault-ssh-helper
$ make bootstrap
$ make
$ make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に設定ファイルを作ります。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vi /etc/vault-ssh-helper.d/config.hcl
vault_addr=&amp;quot;http://127.0.0.1:8200&amp;quot;
ssh_mount_point=&amp;quot;ssh&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;PAMの設定を変更します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo vi /etc/pam.d/sshd
- auth       substack     password-auth
+ auth requisite pam_exec.so quiet expose_authtok log=/tmp/vaultssh.log /usr/local/bin/vault-ssh-helper -config-file=/etc/vault-ssh-helper.d/config.hcl
+ auth optional pam_unix.so no_set_pass use_first_pass nodelay
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後に /etc/ssh/sshd_config の以下の3つを変更しておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ChallengeResponseAuthentication yes
UsePAM yes
PasswordAuthentication no
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;one-time-password-type&#34;&gt;One Time Password Type&lt;/h3&gt;

&lt;p&gt;それでは dev mode でOTPを試してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export VAULT_ADDR=&#39;http://127.0.0.1:8200&#39;
$ vault mount ssh
Successfully mounted &#39;ssh&#39; at &#39;ssh&#39;!
$ vault write ssh/roles/otp_key_role \
    key_type=otp \
    default_user=centos \
    cidr_list=127.0.0.0/8,192.168.11.0/24
Success! Data written to: ssh/roles/otp_key_role
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;以上がOTPの設定です。
One-Time-Tokenを作ってみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vault write ssh/creds/otp_key_role ip=127.0.0.1
Key             Value
lease_id        ssh/creds/otp_key_role/ab69a54e-f845-d29a-0627-a3971cacb7b7
lease_duration  2592000
lease_renewable false
key             0c9e7c24-64bb-8c71-3aa4-fdff88b31fde
key_type        otp
port            22
username        centos
ip              127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ここで指定する &lt;code&gt;ip&lt;/code&gt; は前述の&lt;code&gt;cidr_list&lt;/code&gt;の範囲であれば何でも良いです。
出力結果の&lt;code&gt;key&lt;/code&gt;がOTPです。&lt;/p&gt;

&lt;p&gt;試してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh centos@localhost 
Password: 0c9e7c24-64bb-8c71-3aa4-fdff88b31fde
[centos@localhost ~]$ exit
$ ssh centos@localhost
Password: 0c9e7c24-64bb-8c71-3aa4-fdff88b31fde
Password: 
Password: 
Permission denied (publickey,gssapi-keyex,gssapi-with-mic,keyboard-interactive).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このように一度使ってしまったOTPは再度利用できません。&lt;/p&gt;

&lt;p&gt;次に他のユーザでログインできるOTP作ってみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vault write ssh/creds/otp_key_role ip=127.0.0.1 username=vagrant                                                                                                                                                                           
Key             Value
lease_id        ssh/creds/otp_key_role/f49cfd4e-907d-5cf7-1d80-c9fc932b5ad9
lease_duration  2592000
lease_renewable false
key_type        otp
port            22
username        vagrant
ip              127.0.0.1
key             21a90e3b-fe47-4766-af2e-fcb0093c1864
$ ssh vagrant@localhost
Password: 21a90e3b-fe47-4766-af2e-fcb0093c1864
[vagrant@localhost ~]$ 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;毎回 &lt;code&gt;vault write&lt;/code&gt; コマンドでOTPを作るのはめんどくさいという方は &lt;code&gt;vault ssh&lt;/code&gt; と言うコマンドが用意されています。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vault ssh -role otp_key_role centos@localhost
OTP for the session is 28535dc6-7b50-af74-b37f-4b6ebedcf50c
[Note: Install &#39;sshpass&#39; to automate typing in OTP]
Password: 28535dc6-7b50-af74-b37f-4b6ebedcf50c
Last login: Thu Oct 22 23:55:06 2015 from localhost
[centos@localhost ~]$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;書いてある通り &lt;code&gt;sshpass&lt;/code&gt; を入れておけばOTPの入力を省略できます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo yum install -y epel-release
$ sudo yum install -y sshpass
$ vault ssh -role otp_key_role centos@localhost
Last login: Thu Oct 22 23:59:23 2015 from localhost
[centos@localhost ~]$
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dynamic-type&#34;&gt;Dynamic Type&lt;/h3&gt;

&lt;p&gt;Dynamic Typeは公開鍵を使用して認証として利用する方式です。
仕組みとしては、Vault サーバがログイン先のサーバ・ユーザの authorized_keys に一時的に生成した公開鍵をインジェクトするというものです。&lt;/p&gt;

&lt;p&gt;インジェクトするためにSSHを利用するので、あらかじめVaultに追加した秘密鍵で生成した公開鍵を各サーバに登録しておく必要があります。
また、Vault Serverがインジェクトに使用するユーザが sudo が使える権限を付与する必要があります。&lt;/p&gt;

&lt;p&gt;また、今回はパスワード認証を行わないので&lt;code&gt;ChallengeResponseAuthentication&lt;/code&gt;は&lt;code&gt;no&lt;/code&gt;にしておきましょう。&lt;/p&gt;

&lt;p&gt;最初に秘密鍵・公開鍵ペアの作成し秘密鍵をVaultに登録します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ openssl genrsa -out shared_key.pem 2048                                                                                                                                                                                                                 
Generating RSA private key, 2048 bit long modulus
............................................................+++
.........................+++
e is 65537 (0x10001)
[vagrant@localhost ~]$ vault write ssh/keys/dev_key key=@shared_key.pem
Success! Data written to: ssh/keys/dev_key
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;各サーバの sudoers に以下の設定を入れます。
Vault Serverがログインに利用するユーザは &lt;code&gt;vaultadmin&lt;/code&gt; とします。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ ssh-keygen -f shared_key.pem -y | sudo tee -a /home/vaultadmin/.ssh/authorized_keys
[vagrant@localhost ~]$ visudo
vaultadmin   ALL=(ALL)NOPASSWD: ALL
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最後にDynamic Key Roleを有効化します。
&lt;code&gt;admin_user&lt;/code&gt; は Vault serverがログインに使用するユーザである &lt;code&gt;vaultadmin&lt;/code&gt; を指定します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ vault write ssh/roles/dynamic_key_role \
    key_type=dynamic \
    key=dev_key \
    admin_user=vaultadmin \
    default_user=centos \
    cidr_list=127.0.0.0/8,192.168.33.0/24
Success! Data written to: ssh/roles/dynamic_key_role
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;では、キーペアを作成してみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ vault write ssh/creds/dynamic_key_role ip=127.0.0.1
Key             Value
lease_id        ssh/creds/dynamic_key_role/657543b6-26ee-370c-d67c-bf0965171afd
lease_duration  2592000
lease_renewable true
ip              127.0.0.1
key             -----BEGIN RSA PRIVATE KEY-----
MIICXQIBAAKBgQC1fPQ1wA0eThBxPCu3WwCBWontKrrYaiOXJkjGkPl+fFSBwIP3
7WUzfVrI9oYEeFisRvbsi1NjMeAFpHwC5vGbqGWd8UGUZW5PDyfoPOnj9jsbUd5U
kPz1nZfxKke73WpXShyqtO3nIiF3BU6XbBSJuOuvDCC/P5ZCL/vSYHCjcQIDAQAB
AoGBAKgUVDtfZQbW92VXe4kxL3Oc/TX3p9l72wBGBYpYg6f/z2fnepDnfB1GkAik
P5PuPPk4M8D4e77XVwkCv5MUfVbC52fQjpVktKsdvt4TMx9r3RrIm/xEkxRNJSr1
vYIXcfBzh49s3CwPrMAOXugE0/1lx2dVjovS8BZbv9wBm9vBAkEAzj4j4PWcSMIn
5MPqyQ8cvWAPdOAKIDXStzTBZ3drHTnE1s2AlSVuBMTnDAvIBXHGpy8lTsysHGqk
OxDWhVseGQJBAOFF7jsSdaU9nhoUk+9wmK/iXrTbjJgzRegmBPnVRaZQNurxcv9p
wQqqMTynP0rIBWuun32wyxQJiS4y9jbwqxkCQQDC7rrMqnhv0IsSTxa/uHfqijux
tPv9G8IxBTzzxUxJkEt61zt8PKdy/ISAvzXr53Dinc3+X7chGK5nYW/RFaEpAkB8
2shd/y4rJkqRQ+R2Kd7GZN1+ucxjss9FCoVpfpX6xqyZbLcC7rcqVQezCTMgHFo8
w2zsOedkNKDOdTpXWu5JAkBWsUq1pLv3B7L5ZgGzE3lIzCNJAsrzoyB+VCp3LsxR
XQ4i1krzcUCyh3g58CdN7mS82kkrQ9iGkTkbu7B6JmVZ
-----END RSA PRIVATE KEY-----
key_type        dynamic
port            22
username        centos
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;key&lt;/code&gt;が生成された秘密鍵です。ファイルに保存します。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ vi dynkey.pem
-----BEGIN RSA PRIVATE KEY-----
MIICXQIBAAKBgQC1fPQ1wA0eThBxPCu3WwCBWontKrrYaiOXJkjGkPl+fFSBwIP3
7WUzfVrI9oYEeFisRvbsi1NjMeAFpHwC5vGbqGWd8UGUZW5PDyfoPOnj9jsbUd5U
kPz1nZfxKke73WpXShyqtO3nIiF3BU6XbBSJuOuvDCC/P5ZCL/vSYHCjcQIDAQAB
AoGBAKgUVDtfZQbW92VXe4kxL3Oc/TX3p9l72wBGBYpYg6f/z2fnepDnfB1GkAik
P5PuPPk4M8D4e77XVwkCv5MUfVbC52fQjpVktKsdvt4TMx9r3RrIm/xEkxRNJSr1
vYIXcfBzh49s3CwPrMAOXugE0/1lx2dVjovS8BZbv9wBm9vBAkEAzj4j4PWcSMIn
5MPqyQ8cvWAPdOAKIDXStzTBZ3drHTnE1s2AlSVuBMTnDAvIBXHGpy8lTsysHGqk
OxDWhVseGQJBAOFF7jsSdaU9nhoUk+9wmK/iXrTbjJgzRegmBPnVRaZQNurxcv9p
wQqqMTynP0rIBWuun32wyxQJiS4y9jbwqxkCQQDC7rrMqnhv0IsSTxa/uHfqijux
tPv9G8IxBTzzxUxJkEt61zt8PKdy/ISAvzXr53Dinc3+X7chGK5nYW/RFaEpAkB8
2shd/y4rJkqRQ+R2Kd7GZN1+ucxjss9FCoVpfpX6xqyZbLcC7rcqVQezCTMgHFo8
w2zsOedkNKDOdTpXWu5JAkBWsUq1pLv3B7L5ZgGzE3lIzCNJAsrzoyB+VCp3LsxR
XQ4i1krzcUCyh3g58CdN7mS82kkrQ9iGkTkbu7B6JmVZ
-----END RSA PRIVATE KEY-----
[vagrant@localhost ~]$ chmod 400 dynkey.pem
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存した鍵を利用し、ログインしてみます。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ ssh -i dynkey.pem centos@localhost
[centos@localhost ~]$ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;こちらはRevokeするか、&lt;code&gt;lease_duration&lt;/code&gt;が経過するまで鍵を使用することが出来ます。
Revokeは次のように行います。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[vagrant@localhost ~]$ vault revoke ssh/creds/dynamic_key_role/657543b6-26ee-370c-d67c-bf0965171afd
Key revoked with ID &#39;ssh/creds/dynamic_key_role/594ac177-f4b2-b840-31ab-4cea0b15e18a&#39;.
[vagrant@localhost ~]$ ssh -i dynkey.pem centos@localhost
Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;まとめ&#34;&gt;まとめ&lt;/h2&gt;

&lt;p&gt;Vaultを利用することでSSHの鍵の管理をVaultに任せてしまうことができるので、各サーバに鍵の展開を行う必要はなくなりそうです。
今回は使ってみることを優先してみましたが、実際に利用する際は&lt;a href=&#34;https://vaultproject.io/docs/auth/index.html&#34;&gt;Auth Backends&lt;/a&gt;や&lt;a href=&#34;https://vaultproject.io/docs/audit/index.html&#34;&gt;Audit Backends&lt;/a&gt;の設定が必要です。&lt;/p&gt;

&lt;h2 id=&#34;参考文献&#34;&gt;参考文献&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://vaultproject.io/docs/secrets/ssh/index.html&#34;&gt;Secret Backend: SSH - Vault by HashiCorp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Consul, Consul TemplateとPrometheusで行う簡易メトリクス収集</title>
      <link>https://kjmkznr.github.io/post/consul-prometheus/</link>
      <pubDate>Mon, 05 Oct 2015 22:09:00 +0900</pubDate>
      
      <guid>https://kjmkznr.github.io/post/consul-prometheus/</guid>
      <description>&lt;p&gt;Consulをサービスディスカバリに使い、Consul TemplateでPrometheusの設定ファイルを生成するという構成で、ノードが増加しても自動的にPrometheusの収集対象が増えるような構成を作ってみました。&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;prometheus&#34;&gt;Prometheus&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://prometheus.io/&#34;&gt;Prometheus&lt;/a&gt;はSoundCloudが開発を行っている監視システムです。
Graphiteと違いPrometheusはPull型のアーキテクチャをとっています。Prometheusサーバがexporterと呼ばれる所謂エージェントに値を取りに行く形です。&lt;/p&gt;

&lt;p&gt;Push型に比べれば、Pull型は負荷が集中するので収集できるノード数は少なくなると思います。
ただ、Push型は何らかの理由でメトリクスデータをPushできなくなったときに、検知が困難になるというデメリットもあります。
Pull型であれば取れなかったことが検知でき、その理由も調べることが可能です。&lt;/p&gt;

&lt;p&gt;時系列データストアは自前の物を利用しているようですが、OpenTSDBも利用可能らしいです。
独自のクエリをサポートしており、収集したデータを加工して表示することも出来ます。
グラフ表示機能はPrometheusサーバにも簡易的な物が用意されていますが、 Rails で書かれた PromDash というものを利用するのが良さそうです。
その他、Push型のデータを収集するための pushgateway や、アラートを出すための alertmanager もあります。&lt;/p&gt;

&lt;p&gt;今回は Prometheus Server と node_exporter を利用します。&lt;/p&gt;

&lt;h2 id=&#34;動作環境&#34;&gt;動作環境&lt;/h2&gt;

&lt;p&gt;今回は AWS EC2 を使用します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Amazon EC2&lt;/li&gt;
&lt;li&gt;Amazon Linux 2015.03.1

&lt;ul&gt;
&lt;li&gt;Prometheus 0.14.0&lt;/li&gt;
&lt;li&gt;node_exporter 0.9.0&lt;/li&gt;
&lt;li&gt;Consul 0.5.2&lt;/li&gt;
&lt;li&gt;Consul Template 0.10.0&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tags&#34;&gt;Tags&lt;/h3&gt;

&lt;p&gt;EC2のタグは以下のようにしておきます。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;server1&lt;/td&gt;
&lt;td&gt;consul-leader: True&lt;/td&gt;
&lt;td&gt;Consul Server, Prometheus Server&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;node1&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Consul Agent, node_exporter&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;node2&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;Consul Agent, node_exporter&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;ports&#34;&gt;Ports&lt;/h3&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Program&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Prometheus Server&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;9090&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;node_exporter&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;9100&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Consul Server&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;8300, 8301, 8400, 8500, 8600&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;UDP&lt;/td&gt;
&lt;td&gt;8600&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Consul Agent&lt;/td&gt;
&lt;td&gt;TCP&lt;/td&gt;
&lt;td&gt;8400, 8500&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;UDP&lt;/td&gt;
&lt;td&gt;8600&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;consul-server-と-consul-template-prometheusの設定&#34;&gt;Consul Server と Consul template, Prometheusの設定&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://consul.io/&#34;&gt;Consul&lt;/a&gt;はPrometheusに比べれば有名なので説明は割愛。
インストール手順も割愛。&lt;/p&gt;

&lt;p&gt;まずは、ConsulとConsul Templateが起動するようにします。
Consul Serverのアドレスを取得するためにEC2のタグを参照するので、IAM Roleを付けておきます。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{
    &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;,
    &amp;quot;Statement&amp;quot;: [
        {
            &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;,
            &amp;quot;Action&amp;quot;: [
                &amp;quot;ec2:DescribeInstances&amp;quot;
            ],
            &amp;quot;Resource&amp;quot;: [
                &amp;quot;arn:aws:ec2:::*&amp;quot;
            ]
        }
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;consul-server&#34;&gt;Consul Server&lt;/h3&gt;

&lt;p&gt;Upstart を使用し Consul Serverを起動させます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/init/consul_server.conf
```
description &amp;ldquo;Consul&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;start on runlevel [2345]
stop on runlevel [!2345]&lt;/p&gt;

&lt;p&gt;respawn&lt;/p&gt;

&lt;p&gt;script
  if [ -f &amp;ldquo;/etc/service/consul&amp;rdquo; ]; then
    . /etc/service/consul
  fi&lt;/p&gt;

&lt;p&gt;# Make sure to use all our CPUs, because Consul can block a scheduler thread
  export GOMAXPROCS=&lt;code&gt;nproc&lt;/code&gt;
  exec /usr/local/bin/consul agent &lt;br /&gt;
   -server &lt;br /&gt;
   -data-dir=&amp;ldquo;/var/lib/consul/&amp;rdquo; &lt;br /&gt;
   -bootstrap-expect １ &lt;br /&gt;
   -config-dir=&amp;ldquo;/etc/consul.d/&amp;rdquo; &lt;br /&gt;
   ${CONSUL_FLAGS} &lt;br /&gt;
   &amp;gt;&amp;gt;/var/log/consul.log 2&amp;gt;&amp;amp;1
end script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
```bash
$ sudo initctl start consul_server
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;consul-template&#34;&gt;Consul Template&lt;/h3&gt;

&lt;p&gt;Consul templateはConsulのクラスタ情報を元にファイルを生成してくれます。
クラスタに変化があると、ファイルを更新し、指定したコマンドを実行してくれます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/init/consul_template.conf
```
description &amp;ldquo;Consul template&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;start on runlevel [2345]
stop on runlevel [!2345]&lt;/p&gt;

&lt;p&gt;respawn&lt;/p&gt;

&lt;p&gt;script
  exec /usr/local/bin/consul-template &lt;br /&gt;
      -consul 127.0.0.1:8500 &lt;br /&gt;
      -template &amp;ldquo;/opt/templates/prometheus.ctmpl:/etc/prometheus/prometheus.yml:initctl restart prometheus&amp;rdquo; &amp;gt;&amp;gt; /var/log/ctemplate.log 2&amp;gt;&amp;amp;1
end script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
```bash
$ sudo initctl start consul_template
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;prometheus-1&#34;&gt;Prometheus&lt;/h3&gt;

&lt;h4 id=&#34;テンプレートの用意&#34;&gt;テンプレートの用意&lt;/h4&gt;

&lt;p&gt;Consul TemplateでPrometheusのコンフィグを生成するためテンプレートを用意します。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/opt/templates/prometheus.ctmpl
```
global:
scrape_interval:     1s
evaluation_interval: 3s&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;labels:
    monitor: &amp;lsquo;web monitor&amp;rsquo;&lt;/p&gt;

&lt;p&gt;scrape_configs:
  - job_name: &amp;lsquo;prometheus&amp;rsquo;
    scrape_interval: 10s
    scrape_timeout: 10s
    target_groups:
      - targets: [&amp;lsquo;localhost:9090&amp;rsquo;]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;job_name: &amp;lsquo;web&amp;rsquo;
scrape_interval: 10s
scrape_timeout: 10s
target_groups:

&lt;ul&gt;
&lt;li&gt;targets: [{{range service &amp;ldquo;web&amp;rdquo;}}&amp;lsquo;{{.Address}}:9100&amp;rsquo;,{{end}}]
```&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;service名は &lt;code&gt;web&lt;/code&gt; としています。
この値は後述する Consul Agent とあわせる必要があります。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;scrape_interval&lt;/code&gt;は値を取得する間隔を指定します。&lt;/p&gt;

&lt;p&gt;Prometheus には&lt;a href=&#34;http://prometheus.io/docs/operating/configuration/#consul-sd-configurations-consul_sd_config&#34;&gt;Consulと連携する機能&lt;/a&gt;があるようなのですが、イマイチ使い方が判りませんでした。
これが使えれば、Consul Templateは不要かもしれません。&lt;/p&gt;

&lt;h4 id=&#34;prometheusのインストール&#34;&gt;Prometheusのインストール&lt;/h4&gt;

&lt;p&gt;ここはまあ、普通に。
Ansible Playbookとかにしておくと良いかもしれません。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo yum install golang git mercurial gcc-c++ epel-release
$ sudo yum install --enablerepo=epel protobuf
$ cd /usr/local/src
$ git clone https://github.com/prometheus/prometheus.git
$ cd prometheus
$ make build
$ sudo useradd -s /bin/false -m prometheus
$ sudo mkdir /etc/prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初期の設定ファイルがないと起動しないので、作成しておきます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/prometheus/prometheus.yml
```yaml
global:
scrape_interval:     1s
evaluation_interval: 3s&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;labels:
    monitor: &amp;lsquo;web monitor&amp;rsquo;&lt;/p&gt;

&lt;p&gt;scrape_configs:
  - job_name: &amp;lsquo;prometheus&amp;rsquo;
    scrape_interval: 10s
    scrape_timeout: 10s
    target_groups:
      - targets: [&amp;lsquo;localhost:9090&amp;rsquo;]&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
PrometheusもUpstartで起動させます。

* /etc/init/prometheus.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;description &amp;ldquo;Prometheus&amp;rdquo;&lt;/p&gt;

&lt;p&gt;start on runlevel [2345]
stop on runlevel [!2345]&lt;/p&gt;

&lt;p&gt;respawn&lt;/p&gt;

&lt;p&gt;script
  /usr/local/src/prometheus/prometheus -config.file=/etc/prometheus/prometheus.yml
end script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
```bash
$ sudo initctl start prometheus
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;consul-agentの設定とサービスの定義&#34;&gt;Consul Agentの設定とサービスの定義&lt;/h2&gt;

&lt;h3 id=&#34;consul-agent&#34;&gt;Consul Agent&lt;/h3&gt;

&lt;p&gt;こちらもConsul Serverと同じく Upstart で起動させます。
起動する際に、Consul Serverのアドレスを AWS CLI を使って取得し、クラスタへJoinさせます。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;/etc/init/consul_agent.conf
```
description &amp;ldquo;Consul&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;start on runlevel [2345]
stop on runlevel [!2345]&lt;/p&gt;

&lt;p&gt;respawn&lt;/p&gt;

&lt;p&gt;script
  if [ -f &amp;ldquo;/etc/service/consul&amp;rdquo; ]; then
    . /etc/service/consul
  fi&lt;/p&gt;

&lt;p&gt;# Make sure to use all our CPUs, because Consul can block a scheduler thread
  export GOMAXPROCS=&lt;code&gt;nproc&lt;/code&gt;
  JOIN_TO=&amp;ldquo;$(aws ec2 describe-instances &amp;ndash;query &amp;lsquo;Reservations[].Instances[?State.Name==&lt;code&gt;running&lt;/code&gt;].PrivateIpAddress&amp;rsquo; &amp;ndash;output text &amp;ndash;filters &amp;lsquo;Name=tag:consul-leader,Values=true&amp;rsquo;)&amp;rdquo;&lt;/p&gt;

&lt;p&gt;exec /usr/local/bin/consul agent &lt;br /&gt;
   -data-dir=&amp;ldquo;/var/lib/consul&amp;rdquo; &lt;br /&gt;
   -config-dir=&amp;ldquo;/etc/consul.d&amp;rdquo; &lt;br /&gt;
   -join ${JOIN_TO} &lt;br /&gt;
   ${CONSUL_FLAGS} &lt;br /&gt;
   &amp;gt;&amp;gt;/var/log/consul.log 2&amp;gt;&amp;amp;1
end script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;
ついでにサービスの定義も行います。
Consulは簡易なサービスのチェックも行えるようになっております。

* /etc/consul.d/web.json
```json
{
  &amp;quot;services&amp;quot;:[
    {
      &amp;quot;name&amp;quot;: &amp;quot;Web&amp;quot;,
      &amp;quot;port&amp;quot;: 80,
      &amp;quot;check&amp;quot;: {
        &amp;quot;script&amp;quot;: &amp;quot;curl -s http://localhost/ -o /dev/null&amp;quot;,
        &amp;quot;interval&amp;quot;: &amp;quot;10s&amp;quot;
      },
      &amp;quot;tags&amp;quot;: [&amp;quot;http&amp;quot;]
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sudo initctl start consul_agent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これらを組み込んだAMIを作っておいて、あとはインスタンスを起動していけば自動的にどんどん監視対象が増えていきます。&lt;/p&gt;

&lt;h2 id=&#34;promdash&#34;&gt;PromDash&lt;/h2&gt;

&lt;p&gt;PromDashというダッシュボード的なWebインターフェイスが提供されています。
PromDashはRailsで出来ており、動かすのがめんどうなので、Dockerで動かすのがオススメです。&lt;/p&gt;

&lt;p&gt;PromDash内のデータベースには、PrometheusサーバのURLと表示するグラフに関する情報のみ記録されてるだけなので、Dockerコンテナが消えても、収集したメトリクスは消えません。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker pull prom/promdash
$ docker run -v /tmp/prom:/tmp/prom -e DATABASE_URL=sqlite3:/tmp/prom/file.sqlite3 prom/promdash ./bin/rake db:migrate
$ docker run -d -p 3000:3000 -v /tmp/prom:/tmp/prom -e DATABASE_URL=sqlite3:/tmp/prom/file.sqlite3 prom/promdash
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;所感&#34;&gt;所感&lt;/h2&gt;

&lt;p&gt;監視を行うのに一通りの機能は揃っていますが、長期的に運用するときにどのような問題が出るか不透明な点はあります。
短期間使ってみた感じでは、短期間のメトリクス収集にはとても有用な感じがしました。
クエリによる値の集計なども簡単にできてよいです。&lt;/p&gt;

&lt;p&gt;プロダクション環境では本格的に使ったわけではありませんが、イベントなどで使うインフラのメトリクス収集に何回か利用しています。&lt;/p&gt;

&lt;p&gt;使い分けとしては、インスタントにメトリクスをとりたい場合は pcp + Vector, 短期間のメトリクス収集には Prometheus, 長期的な監視やメトリクス収集は Zabbix や Sensu + Graphite がよいかなと。&lt;/p&gt;

&lt;p&gt;ほんとはデータの管理とかやりたくないので、Mackerel, Datadog が安くなればいいですね。&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
